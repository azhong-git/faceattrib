{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "# definte relu6\n",
    "from tensorflow.python.keras import backend as K\n",
    "def relu6(x):\n",
    "    return K.relu(x, max_value=6)\n",
    "\n",
    "# model = load_model('models/mobilenet_1.0_224_2018_06_18_16_08_08/face_attrib_mobilenet_1.0_224.10-0.18-0.17.hdf5', \n",
    "#                    custom_objects={'relu6': relu6})\n",
    "# input_shape = (224, 224)\n",
    "# model = load_model('models/mobilenet_1.0_192_2018_06_19_14_57_50/face_attrib_mobilenet_1.0_192.09-0.19-0.18.hdf5', \n",
    "#                    custom_objects={'relu6': relu6})\n",
    "# input_shape = (192, 192)\n",
    "# model = load_model('models/mobilenet_0.75_224_2018_06_20_11_40_04/face_attrib_mobilenet_0.75_224.11-0.19-0.17.hdf5', \n",
    "#                    custom_objects={'relu6': relu6})\n",
    "# input_shape = (224, 224)\n",
    "# model = load_model('models/mobilenet_0.5_224_2018_06_20_17_04_49/face_attrib_mobilenet_0.5_224.14-0.19-0.18.hdf5', \n",
    "#                    custom_objects={'relu6': relu6})\n",
    "# input_shape = (224, 224)\n",
    "\n",
    "# the following gives 45 outputs including ethnicities\n",
    "model = load_model('models/mobilenet_1.0_224_2018_06_13_17_27_59/face_attrib_mobilenet_1.0_224.25-0.32-0.08.hdf5', \n",
    "                   custom_objects={'relu6': relu6})\n",
    "input_shape = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 35s 2ms/step\n",
      "20000/20000 [==============================] - 32s 2ms/step\n",
      "20000/20000 [==============================] - 32s 2ms/step\n",
      "20000/20000 [==============================] - 31s 2ms/step\n",
      "20000/20000 [==============================] - 31s 2ms/step\n",
      "20000/20000 [==============================] - 31s 2ms/step\n",
      "20000/20000 [==============================] - 31s 2ms/step\n",
      "20000/20000 [==============================] - 31s 2ms/step\n",
      "2770/2770 [==============================] - 5s 2ms/step\n",
      "19867/19867 [==============================] - 31s 2ms/step\n",
      "19962/19962 [==============================] - 31s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from data import load_attributes\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "data_dir = '/data/celeba'\n",
    "train_images_path = os.path.join(data_dir, 'Img/img_align_celeba_crop_middle_train/')\n",
    "val_images_path = os.path.join(data_dir, 'Img/img_align_celeba_crop_middle_val/')\n",
    "test_images_path = os.path.join(data_dir, 'Img/img_align_celeba_crop_middle_test/')\n",
    "attributes_path = os.path.join('.', 'celeba_header_lines_40.p')\n",
    "multilabel_dict, labels = load_attributes(attributes_path)\n",
    "import cv2\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = image / 255.0\n",
    "    image = image - 0.5\n",
    "    image = image * 2.0\n",
    "    return image\n",
    "\n",
    "def get_data_in_X_Y(images_path, multilabel_dict):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for img_path in os.listdir(images_path):\n",
    "        real_img_path = os.path.join(images_path, img_path)\n",
    "        img = image.load_img(real_img_path, target_size=input_shape)\n",
    "        x = image.img_to_array(img)\n",
    "        # x = cv2.cvtColor(x, cv2.COLOR_RGB2BGR)\n",
    "        x = preprocess_image(x)\n",
    "        if img_path in multilabel_dict:\n",
    "            X.append(x)\n",
    "            Y.append(multilabel_dict[img_path])\n",
    "    X=np.array(X)\n",
    "    Y=np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def eval_image_path(images_path, multilabel_dict, model, limit = 20000):\n",
    "    img_paths = sorted(os.listdir(images_path))\n",
    "    start = 0\n",
    "    end = limit\n",
    "    Y_predicted_all = []\n",
    "    Y_gt_all = []\n",
    "    images = []\n",
    "    while start < len(img_paths):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for img_path in img_paths[start:end]:\n",
    "            real_img_path = os.path.join(images_path, img_path)\n",
    "            img = image.load_img(real_img_path, target_size=input_shape)\n",
    "            x = image.img_to_array(img)\n",
    "            x = cv2.cvtColor(x, cv2.COLOR_RGB2BGR)\n",
    "            x = preprocess_image(x)\n",
    "            if img_path in multilabel_dict:\n",
    "                X.append(x)\n",
    "                Y.append(multilabel_dict[img_path])\n",
    "                images.append(img_path)\n",
    "        Y_predicted = model.predict(np.array(X), batch_size=64, verbose=1)\n",
    "        Y_gt_all.extend(Y)\n",
    "        Y_predicted_all.extend(Y_predicted)\n",
    "        start = start + limit\n",
    "        end = end + limit\n",
    "    return Y_predicted_all, Y_gt_all, images\n",
    "\n",
    "train_Y_predicted, train_Y_gt, train_img_paths = eval_image_path(train_images_path + 'all', multilabel_dict, model)\n",
    "train_Y_predicted_rounded = np.round(train_Y_predicted)\n",
    "val_Y_predicted, val_Y_gt, val_img_paths = eval_image_path(val_images_path + 'all', multilabel_dict, model)\n",
    "val_Y_predicted_rounded = np.round(val_Y_predicted)\n",
    "test_Y_predicted, test_Y_gt, test_img_paths = eval_image_path(test_images_path + 'all', multilabel_dict, model)\n",
    "test_Y_predicted_rounded = np.round(test_Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- validation accuracies -------------------------\n",
      "mean: 0.6762\n",
      "5_o_Clock_Shadow, 0.2738\n",
      "Arched_Eyebrows, 0.5033\n",
      "Attractive, 0.5959\n",
      "Bags_Under_Eyes, 0.2703\n",
      "Bald, 0.7223\n",
      "Bangs, 0.8396\n",
      "Big_Lips, 0.6756\n",
      "Big_Nose, 0.5253\n",
      "Black_Hair, 0.5457\n",
      "Blond_Hair, 0.6505\n",
      "Blurry, 0.8481\n",
      "Brown_Hair, 0.7458\n",
      "Bushy_Eyebrows, 0.5111\n",
      "Chubby, 0.6848\n",
      "Double_Chin, 0.8741\n",
      "Eyeglasses, 0.8589\n",
      "Goatee, 0.8765\n",
      "Gray_Hair, 0.8229\n",
      "Heavy_Makeup, 0.5864\n",
      "High_Cheekbones, 0.6206\n",
      "Male, 0.3940\n",
      "Mouth_Slightly_Open, 0.8588\n",
      "Mustache, 0.9153\n",
      "Narrow_Eyes, 0.6488\n",
      "No_Beard, 0.9022\n",
      "Oval_Face, 0.4288\n",
      "Pale_Skin, 0.5132\n",
      "Pointy_Nose, 0.3889\n",
      "Receding_Hairline, 0.7382\n",
      "Rosy_Cheeks, 0.7558\n",
      "Sideburns, 0.8258\n",
      "Smiling, 0.8617\n",
      "Straight_Hair, 0.6789\n",
      "Wavy_Hair, 0.5726\n",
      "Wearing_Earrings, 0.6996\n",
      "Wearing_Hat, 0.9574\n",
      "Wearing_Lipstick, 0.8880\n",
      "Wearing_Necklace, 0.5427\n",
      "Wearing_Necktie, 0.7751\n",
      "Young, 0.6710\n",
      "-------------------- test accuracies -------------------------\n",
      "mean: 0.6837\n",
      "5_o_Clock_Shadow, 0.3145\n",
      "Arched_Eyebrows, 0.5114\n",
      "Attractive, 0.6024\n",
      "Bags_Under_Eyes, 0.2658\n",
      "Bald, 0.7322\n",
      "Bangs, 0.8267\n",
      "Big_Lips, 0.5979\n",
      "Big_Nose, 0.5310\n",
      "Black_Hair, 0.5489\n",
      "Blond_Hair, 0.6354\n",
      "Blurry, 0.8620\n",
      "Brown_Hair, 0.7993\n",
      "Bushy_Eyebrows, 0.5231\n",
      "Chubby, 0.6954\n",
      "Double_Chin, 0.8781\n",
      "Eyeglasses, 0.8654\n",
      "Goatee, 0.8955\n",
      "Gray_Hair, 0.8591\n",
      "Heavy_Makeup, 0.5759\n",
      "High_Cheekbones, 0.6384\n",
      "Male, 0.3954\n",
      "Mouth_Slightly_Open, 0.8591\n",
      "Mustache, 0.9303\n",
      "Narrow_Eyes, 0.6599\n",
      "No_Beard, 0.9136\n",
      "Oval_Face, 0.4412\n",
      "Pale_Skin, 0.5328\n",
      "Pointy_Nose, 0.4134\n",
      "Receding_Hairline, 0.7579\n",
      "Rosy_Cheeks, 0.7555\n",
      "Sideburns, 0.8380\n",
      "Smiling, 0.8691\n",
      "Straight_Hair, 0.6556\n",
      "Wavy_Hair, 0.6003\n",
      "Wearing_Earrings, 0.6796\n",
      "Wearing_Hat, 0.9634\n",
      "Wearing_Lipstick, 0.9043\n",
      "Wearing_Necklace, 0.5373\n",
      "Wearing_Necktie, 0.7975\n",
      "Young, 0.6866\n",
      "-------------------- train accuracies -------------------------\n",
      "mean: 0.6794\n",
      "5_o_Clock_Shadow, 0.2957\n",
      "Arched_Eyebrows, 0.5097\n",
      "Attractive, 0.6097\n",
      "Bags_Under_Eyes, 0.2670\n",
      "Bald, 0.7284\n",
      "Bangs, 0.8316\n",
      "Big_Lips, 0.6285\n",
      "Big_Nose, 0.5259\n",
      "Black_Hair, 0.5427\n",
      "Blond_Hair, 0.6421\n",
      "Blurry, 0.8515\n",
      "Brown_Hair, 0.7763\n",
      "Bushy_Eyebrows, 0.5325\n",
      "Chubby, 0.6736\n",
      "Double_Chin, 0.8796\n",
      "Eyeglasses, 0.8646\n",
      "Goatee, 0.8837\n",
      "Gray_Hair, 0.8399\n",
      "Heavy_Makeup, 0.5927\n",
      "High_Cheekbones, 0.6257\n",
      "Male, 0.3997\n",
      "Mouth_Slightly_Open, 0.8581\n",
      "Mustache, 0.9238\n",
      "Narrow_Eyes, 0.6448\n",
      "No_Beard, 0.9029\n",
      "Oval_Face, 0.4347\n",
      "Pale_Skin, 0.5235\n",
      "Pointy_Nose, 0.3935\n",
      "Receding_Hairline, 0.7424\n",
      "Rosy_Cheeks, 0.7573\n",
      "Sideburns, 0.8264\n",
      "Smiling, 0.8673\n",
      "Straight_Hair, 0.6631\n",
      "Wavy_Hair, 0.5840\n",
      "Wearing_Earrings, 0.6971\n",
      "Wearing_Hat, 0.9585\n",
      "Wearing_Lipstick, 0.8951\n",
      "Wearing_Necklace, 0.5452\n",
      "Wearing_Necktie, 0.7806\n",
      "Young, 0.6779\n"
     ]
    }
   ],
   "source": [
    "def get_accuracies(predicted, gt):\n",
    "    # assert predicted.shape == gt.shape, '{} != {}'.format(predicted.shape, gt.shape) \n",
    "    num_cols = len(predicted[0])-5\n",
    "    num_rows = len(predicted)\n",
    "    accuracies = []\n",
    "    for i in range(num_cols):\n",
    "        accuracies.append(0)\n",
    "    for j in range(num_rows):\n",
    "        for i in range(num_cols):\n",
    "            if predicted[j][i+5] == gt[j][i]:\n",
    "                accuracies[i] += 1\n",
    "    for i in range(num_cols):\n",
    "        accuracies[i] = accuracies[i] / 1.0 / num_rows\n",
    "    return accuracies\n",
    "\n",
    "val_accuracies = get_accuracies(val_Y_predicted_rounded, val_Y_gt)\n",
    "print('-------------------- validation accuracies -------------------------')\n",
    "print('mean: {:.04f}'.format(np.mean(val_accuracies)))\n",
    "for i in range(len(labels)):\n",
    "    print('{}, {:.04f}'.format(labels[i], val_accuracies[i]))\n",
    "test_accuracies = get_accuracies(test_Y_predicted_rounded, test_Y_gt)\n",
    "print('-------------------- test accuracies -------------------------')\n",
    "print('mean: {:.04f}'.format(np.mean(test_accuracies)))\n",
    "for i in range(len(labels)):\n",
    "    print('{}, {:.04f}'.format(labels[i], test_accuracies[i]))\n",
    "train_accuracies = get_accuracies(train_Y_predicted_rounded, train_Y_gt)\n",
    "print('-------------------- train accuracies -------------------------')\n",
    "print('mean: {:.04f}'.format(np.mean(train_accuracies)))\n",
    "for i in range(len(labels)):\n",
    "    print('{}, {:.04f}'.format(labels[i], train_accuracies[i]))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_images_path = os.path.join('/data/lfw/lfw_all_funneled_face_crop_l_0.3_r_0.3_t_0.4_d_0.2/all/')\n",
    "new_attributes_path = os.path.join('.', 'lfw_header_lines_40.p')\n",
    "new_multilabel_dict, new_labels = load_attributes(new_attributes_path)\n",
    "new_X, new_Y_gt = get_data_in_X_Y(new_images_path, new_multilabel_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13143/13143 [==============================] - 22s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "new_Y_predicted = model.predict(new_X, batch_size=64, verbose=1)\n",
    "new_Y_predicted_rounded = np.round(new_Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- test accuracies -------------------------\n",
      "mean: 0.7384\n",
      "5_o_Clock_Shadow, 0.6203\n",
      "Arched_Eyebrows, 0.7698\n",
      "Attractive, 0.6747\n",
      "Bags_Under_Eyes, 0.6659\n",
      "Bald, 0.9085\n",
      "Bangs, 0.8598\n",
      "Big_Lips, 0.6674\n",
      "Big_Nose, 0.5404\n",
      "Black_Hair, 0.8745\n",
      "Blond_Hair, 0.9610\n",
      "Blurry, 0.8304\n",
      "Brown_Hair, 0.6669\n",
      "Bushy_Eyebrows, 0.5291\n",
      "Chubby, 0.6743\n",
      "Double_Chin, 0.6810\n",
      "Eyeglasses, 0.9216\n",
      "Goatee, 0.7778\n",
      "Gray_Hair, 0.8535\n",
      "Heavy_Makeup, 0.9340\n",
      "High_Cheekbones, 0.8401\n",
      "Male, 0.9781\n",
      "Mouth_Slightly_Open, 0.7860\n",
      "Mustache, 0.9101\n",
      "Narrow_Eyes, 0.4376\n",
      "No_Beard, 0.7962\n",
      "Oval_Face, 0.4810\n",
      "Pale_Skin, 0.5002\n",
      "Pointy_Nose, 0.3336\n",
      "Receding_Hairline, 0.5225\n",
      "Rosy_Cheeks, 0.8064\n",
      "Sideburns, 0.7043\n",
      "Smiling, 0.8949\n",
      "Straight_Hair, 0.5862\n",
      "Wavy_Hair, 0.5825\n",
      "Wearing_Earrings, 0.8934\n",
      "Wearing_Hat, 0.8974\n",
      "Wearing_Lipstick, 0.9301\n",
      "Wearing_Necklace, 0.8057\n",
      "Wearing_Necktie, 0.6875\n",
      "Young, 0.7526\n"
     ]
    }
   ],
   "source": [
    "new_accuracies = get_accuracies(new_Y_predicted_rounded, new_Y_gt)\n",
    "print('-------------------- test accuracies -------------------------')\n",
    "print('mean: {:.04f}'.format(np.mean(new_accuracies)))\n",
    "for i in range(len(labels)):\n",
    "    print('{}, {:.04f}'.format(labels[i], new_accuracies[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_labels = ['asianindian', 'eastasian', 'african', 'latino', 'caucasian']\n",
    "\n",
    "race_dict = {}\n",
    "def extract_race_dict(predicted_rounded, predicted, img_paths, race_dict):\n",
    "    for i in range(len(predicted)):\n",
    "        one_of_races = False\n",
    "        for j in range(5):\n",
    "            if predicted_rounded[i][j] == 1:\n",
    "                one_of_races = True\n",
    "                if race_labels[j] not in race_dict:\n",
    "                    race_dict[race_labels[j]] = []\n",
    "                race_dict[race_labels[j]].append(img_paths[i])\n",
    "        if one_of_races == False:\n",
    "            j = np.argmax(predicted[i][0:5])\n",
    "            if race_labels[j] not in race_dict:\n",
    "                race_dict[race_labels[j]] = []\n",
    "            race_dict[race_labels[j]].append(img_paths[i])\n",
    "\n",
    "extract_race_dict(train_Y_predicted_rounded, train_Y_predicted, train_img_paths, race_dict)\n",
    "extract_race_dict(val_Y_predicted_rounded, val_Y_predicted, val_img_paths, race_dict)\n",
    "extract_race_dict(test_Y_predicted_rounded, test_Y_predicted, test_img_paths, race_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(race_dict, open('race_dict.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
