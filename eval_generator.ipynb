{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "# definte relu6\n",
    "from tensorflow.python.keras import backend as K\n",
    "def relu6(x):\n",
    "    return K.relu(x, max_value=6)\n",
    "\n",
    "# model = load_model('models/mobilenet_1.0_224_2018_06_18_16_08_08/face_attrib_mobilenet_1.0_224.10-0.18-0.17.hdf5', \n",
    "#                    custom_objects={'relu6': relu6})\n",
    "# input_shape = (224, 224)\n",
    "# model = load_model('models/mobilenet_1.0_192_2018_06_19_14_57_50/face_attrib_mobilenet_1.0_192.09-0.19-0.18.hdf5', \n",
    "#                    custom_objects={'relu6': relu6})\n",
    "# input_shape = (192, 192)\n",
    "# model = load_model('models/mobilenet_0.75_224_2018_06_20_11_40_04/face_attrib_mobilenet_0.75_224.11-0.19-0.17.hdf5', \n",
    "#                    custom_objects={'relu6': relu6})\n",
    "# input_shape = (224, 224)\n",
    "# model = load_model('models/mobilenet_0.5_224_2018_06_20_17_04_49/face_attrib_mobilenet_0.5_224.14-0.19-0.18.hdf5', \n",
    "#                    custom_objects={'relu6': relu6})\n",
    "# input_shape = (224, 224)\n",
    "\n",
    "# the following gives 45 outputs including ethnicities\n",
    "# model = load_model('models/mobilenet_1.0_224_2018_06_13_17_27_59/face_attrib_mobilenet_1.0_224.25-0.32-0.08.hdf5', \n",
    "#                   custom_objects={'relu6': relu6})\n",
    "model = load_model('models/mobilenet_0.5_224_2018_07_26_16_23_42/face_attrib_mobilenet_0.5_224.19-0.17-0.15.hdf5', \n",
    "                  custom_objects={'relu6': relu6})\n",
    "\n",
    "input_shape = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 27s 1ms/step\n",
      "20000/20000 [==============================] - 26s 1ms/step\n",
      "20000/20000 [==============================] - 25s 1ms/step\n",
      "20000/20000 [==============================] - 26s 1ms/step\n",
      "20000/20000 [==============================] - 26s 1ms/step\n",
      "20000/20000 [==============================] - 26s 1ms/step\n",
      "20000/20000 [==============================] - 26s 1ms/step\n",
      "20000/20000 [==============================] - 26s 1ms/step\n",
      "2770/2770 [==============================] - 3s 962us/step\n",
      "19867/19867 [==============================] - 23s 1ms/step\n",
      "19962/19962 [==============================] - 26s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from data import load_attributes\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "data_dir = '/data/celeba'\n",
    "train_images_path = os.path.join(data_dir, 'Img/img_align_celeba_crop_middle_train/')\n",
    "val_images_path = os.path.join(data_dir, 'Img/img_align_celeba_crop_middle_val/')\n",
    "test_images_path = os.path.join(data_dir, 'Img/img_align_celeba_crop_middle_test/')\n",
    "attributes_path = os.path.join('.', 'celeba_header_lines_45.p')\n",
    "multilabel_dict, labels = load_attributes(attributes_path)\n",
    "import cv2\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = image / 255.0\n",
    "    image = image - 0.5\n",
    "    image = image * 2.0\n",
    "    return image\n",
    "\n",
    "def get_data_in_X_Y(images_path, multilabel_dict):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for img_path in os.listdir(images_path):\n",
    "        real_img_path = os.path.join(images_path, img_path)\n",
    "        img = image.load_img(real_img_path, target_size=input_shape)\n",
    "        x = image.img_to_array(img)\n",
    "        # x = cv2.cvtColor(x, cv2.COLOR_RGB2BGR)\n",
    "        x = preprocess_image(x)\n",
    "        if img_path in multilabel_dict:\n",
    "            X.append(x)\n",
    "            Y.append(multilabel_dict[img_path])\n",
    "    X=np.array(X)\n",
    "    Y=np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def eval_image_path(images_path, multilabel_dict, model, limit = 20000):\n",
    "    img_paths = sorted(os.listdir(images_path))\n",
    "    start = 0\n",
    "    end = limit\n",
    "    Y_predicted_all = []\n",
    "    Y_gt_all = []\n",
    "    images = []\n",
    "    while start < len(img_paths):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for img_path in img_paths[start:end]:\n",
    "            real_img_path = os.path.join(images_path, img_path)\n",
    "            img = image.load_img(real_img_path, target_size=input_shape)\n",
    "            x = image.img_to_array(img)\n",
    "            # x = cv2.cvtColor(x, cv2.COLOR_RGB2BGR)\n",
    "            x = preprocess_image(x)\n",
    "            if img_path in multilabel_dict:\n",
    "                X.append(x)\n",
    "                Y.append(multilabel_dict[img_path])\n",
    "                images.append(img_path)\n",
    "        Y_predicted = model.predict(np.array(X), batch_size=64, verbose=1)\n",
    "        Y_gt_all.extend(Y)\n",
    "        Y_predicted_all.extend(Y_predicted)\n",
    "        start = start + limit\n",
    "        end = end + limit\n",
    "    return Y_predicted_all, Y_gt_all, images\n",
    "\n",
    "train_Y_predicted, train_Y_gt, train_img_paths = eval_image_path(train_images_path + 'all', multilabel_dict, model)\n",
    "train_Y_predicted_rounded = np.round(train_Y_predicted)\n",
    "val_Y_predicted, val_Y_gt, val_img_paths = eval_image_path(val_images_path + 'all', multilabel_dict, model)\n",
    "val_Y_predicted_rounded = np.round(val_Y_predicted)\n",
    "test_Y_predicted, test_Y_gt, test_img_paths = eval_image_path(test_images_path + 'all', multilabel_dict, model)\n",
    "test_Y_predicted_rounded = np.round(test_Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- validation accuracies -------------------------\n",
      "mean: 0.9252\n",
      "asianindian, 0.9891\n",
      "eastasian, 0.9820\n",
      "african, 0.9889\n",
      "latino, 0.9579\n",
      "caucasian, 0.9477\n",
      "5_o_Clock_Shadow, 0.9424\n",
      "Arched_Eyebrows, 0.8612\n",
      "Attractive, 0.8187\n",
      "Bags_Under_Eyes, 0.8306\n",
      "Bald, 0.9897\n",
      "Bangs, 0.9609\n",
      "Big_Lips, 0.8174\n",
      "Big_Nose, 0.8352\n",
      "Black_Hair, 0.9219\n",
      "Blond_Hair, 0.9591\n",
      "Blurry, 0.9676\n",
      "Brown_Hair, 0.8627\n",
      "Bushy_Eyebrows, 0.9275\n",
      "Chubby, 0.9568\n",
      "Double_Chin, 0.9667\n",
      "Eyeglasses, 0.9960\n",
      "Goatee, 0.9706\n",
      "Gray_Hair, 0.9801\n",
      "Heavy_Makeup, 0.9274\n",
      "High_Cheekbones, 0.8898\n",
      "Male, 0.9896\n",
      "Mouth_Slightly_Open, 0.9436\n",
      "Mustache, 0.9638\n",
      "Narrow_Eyes, 0.9302\n",
      "No_Beard, 0.9646\n",
      "Oval_Face, 0.7635\n",
      "Pale_Skin, 0.9691\n",
      "Pointy_Nose, 0.7750\n",
      "Receding_Hairline, 0.9477\n",
      "Rosy_Cheeks, 0.9515\n",
      "Sideburns, 0.9752\n",
      "Smiling, 0.9359\n",
      "Straight_Hair, 0.8504\n",
      "Wavy_Hair, 0.8627\n",
      "Wearing_Earrings, 0.9187\n",
      "Wearing_Hat, 0.9908\n",
      "Wearing_Lipstick, 0.9294\n",
      "Wearing_Necklace, 0.8825\n",
      "Wearing_Necktie, 0.9564\n",
      "Young, 0.8833\n",
      "-------------------- test accuracies -------------------------\n",
      "mean: 0.9211\n",
      "asianindian, 0.9842\n",
      "eastasian, 0.9756\n",
      "african, 0.9866\n",
      "latino, 0.9596\n",
      "caucasian, 0.9441\n",
      "5_o_Clock_Shadow, 0.9467\n",
      "Arched_Eyebrows, 0.8405\n",
      "Attractive, 0.8286\n",
      "Bags_Under_Eyes, 0.8474\n",
      "Bald, 0.9910\n",
      "Bangs, 0.9610\n",
      "Big_Lips, 0.7223\n",
      "Big_Nose, 0.8382\n",
      "Black_Hair, 0.9052\n",
      "Blond_Hair, 0.9610\n",
      "Blurry, 0.9643\n",
      "Brown_Hair, 0.8910\n",
      "Bushy_Eyebrows, 0.9262\n",
      "Chubby, 0.9573\n",
      "Double_Chin, 0.9631\n",
      "Eyeglasses, 0.9968\n",
      "Goatee, 0.9760\n",
      "Gray_Hair, 0.9823\n",
      "Heavy_Makeup, 0.9218\n",
      "High_Cheekbones, 0.8835\n",
      "Male, 0.9852\n",
      "Mouth_Slightly_Open, 0.9406\n",
      "Mustache, 0.9694\n",
      "Narrow_Eyes, 0.8735\n",
      "No_Beard, 0.9653\n",
      "Oval_Face, 0.7587\n",
      "Pale_Skin, 0.9720\n",
      "Pointy_Nose, 0.7750\n",
      "Receding_Hairline, 0.9377\n",
      "Rosy_Cheeks, 0.9534\n",
      "Sideburns, 0.9799\n",
      "Smiling, 0.9319\n",
      "Straight_Hair, 0.8438\n",
      "Wavy_Hair, 0.8516\n",
      "Wearing_Earrings, 0.9048\n",
      "Wearing_Hat, 0.9909\n",
      "Wearing_Lipstick, 0.9414\n",
      "Wearing_Necklace, 0.8673\n",
      "Wearing_Necktie, 0.9635\n",
      "Young, 0.8875\n",
      "-------------------- train accuracies -------------------------\n",
      "mean: 0.9339\n",
      "asianindian, 0.9880\n",
      "eastasian, 0.9861\n",
      "african, 0.9917\n",
      "latino, 0.9659\n",
      "caucasian, 0.9605\n",
      "5_o_Clock_Shadow, 0.9502\n",
      "Arched_Eyebrows, 0.8701\n",
      "Attractive, 0.8452\n",
      "Bags_Under_Eyes, 0.8618\n",
      "Bald, 0.9925\n",
      "Bangs, 0.9675\n",
      "Big_Lips, 0.8066\n",
      "Big_Nose, 0.8664\n",
      "Black_Hair, 0.9225\n",
      "Blond_Hair, 0.9640\n",
      "Blurry, 0.9694\n",
      "Brown_Hair, 0.8902\n",
      "Bushy_Eyebrows, 0.9307\n",
      "Chubby, 0.9650\n",
      "Double_Chin, 0.9712\n",
      "Eyeglasses, 0.9977\n",
      "Goatee, 0.9765\n",
      "Gray_Hair, 0.9845\n",
      "Heavy_Makeup, 0.9365\n",
      "High_Cheekbones, 0.8964\n",
      "Male, 0.9915\n",
      "Mouth_Slightly_Open, 0.9492\n",
      "Mustache, 0.9745\n",
      "Narrow_Eyes, 0.9124\n",
      "No_Beard, 0.9687\n",
      "Oval_Face, 0.7894\n",
      "Pale_Skin, 0.9741\n",
      "Pointy_Nose, 0.7954\n",
      "Receding_Hairline, 0.9531\n",
      "Rosy_Cheeks, 0.9601\n",
      "Sideburns, 0.9815\n",
      "Smiling, 0.9415\n",
      "Straight_Hair, 0.8673\n",
      "Wavy_Hair, 0.8740\n",
      "Wearing_Earrings, 0.9231\n",
      "Wearing_Hat, 0.9942\n",
      "Wearing_Lipstick, 0.9456\n",
      "Wearing_Necklace, 0.8903\n",
      "Wearing_Necktie, 0.9660\n",
      "Young, 0.9154\n"
     ]
    }
   ],
   "source": [
    "def get_accuracies(predicted, gt):\n",
    "    # assert predicted.shape == gt.shape, '{} != {}'.format(predicted.shape, gt.shape) \n",
    "    num_cols = len(predicted[0])\n",
    "    num_rows = len(predicted)\n",
    "    accuracies = []\n",
    "    for i in range(num_cols):\n",
    "        accuracies.append(0)\n",
    "    for j in range(num_rows):\n",
    "        for i in range(num_cols):\n",
    "            if predicted[j][i] == gt[j][i]:\n",
    "                accuracies[i] += 1\n",
    "    for i in range(num_cols):\n",
    "        accuracies[i] = accuracies[i] / 1.0 / num_rows\n",
    "    return accuracies\n",
    "\n",
    "val_accuracies = get_accuracies(val_Y_predicted_rounded, val_Y_gt)\n",
    "print('-------------------- validation accuracies -------------------------')\n",
    "print('mean: {:.04f}'.format(np.mean(val_accuracies)))\n",
    "for i in range(len(labels)):\n",
    "    print('{}, {:.04f}'.format(labels[i], val_accuracies[i]))\n",
    "test_accuracies = get_accuracies(test_Y_predicted_rounded, test_Y_gt)\n",
    "print('-------------------- test accuracies -------------------------')\n",
    "print('mean: {:.04f}'.format(np.mean(test_accuracies)))\n",
    "for i in range(len(labels)):\n",
    "    print('{}, {:.04f}'.format(labels[i], test_accuracies[i]))\n",
    "train_accuracies = get_accuracies(train_Y_predicted_rounded, train_Y_gt)\n",
    "print('-------------------- train accuracies -------------------------')\n",
    "print('mean: {:.04f}'.format(np.mean(train_accuracies)))\n",
    "for i in range(len(labels)):\n",
    "    print('{}, {:.04f}'.format(labels[i], train_accuracies[i]))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_predicted_dict = {}\n",
    "race_gt_dict = {}\n",
    "\n",
    "def extend_race_dict(img_paths, predicted, gt, race_predicted_dict, race_gt_dict):\n",
    "    for ind in range(len(img_paths)):\n",
    "        race_predicted_dict[img_paths[ind]] = predicted[ind]\n",
    "        race_gt_dict[img_paths[ind]] = gt[ind]\n",
    "\n",
    "extend_race_dict(train_img_paths, train_Y_predicted, train_Y_gt, race_predicted_dict, race_gt_dict)\n",
    "extend_race_dict(test_img_paths, test_Y_predicted, test_Y_gt, race_predicted_dict, race_gt_dict)\n",
    "extend_race_dict(val_img_paths, val_Y_predicted, val_Y_gt, race_predicted_dict, race_gt_dict)\n",
    "\n",
    "import pickle\n",
    "pickle.dump({'gt': race_gt_dict, 'predicted': race_predicted_dict}, open('race_diff.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_images_path = os.path.join('/data/lfw/lfw_all_funneled_face_crop_l_0.3_r_0.3_t_0.4_d_0.2/all/')\n",
    "new_attributes_path = os.path.join('.', 'lfw_header_lines_40.p')\n",
    "new_multilabel_dict, new_labels = load_attributes(new_attributes_path)\n",
    "new_X, new_Y_gt = get_data_in_X_Y(new_images_path, new_multilabel_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13143/13143 [==============================] - 22s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "new_Y_predicted = model.predict(new_X, batch_size=64, verbose=1)\n",
    "new_Y_predicted_rounded = np.round(new_Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- test accuracies -------------------------\n",
      "mean: 0.7384\n",
      "5_o_Clock_Shadow, 0.6203\n",
      "Arched_Eyebrows, 0.7698\n",
      "Attractive, 0.6747\n",
      "Bags_Under_Eyes, 0.6659\n",
      "Bald, 0.9085\n",
      "Bangs, 0.8598\n",
      "Big_Lips, 0.6674\n",
      "Big_Nose, 0.5404\n",
      "Black_Hair, 0.8745\n",
      "Blond_Hair, 0.9610\n",
      "Blurry, 0.8304\n",
      "Brown_Hair, 0.6669\n",
      "Bushy_Eyebrows, 0.5291\n",
      "Chubby, 0.6743\n",
      "Double_Chin, 0.6810\n",
      "Eyeglasses, 0.9216\n",
      "Goatee, 0.7778\n",
      "Gray_Hair, 0.8535\n",
      "Heavy_Makeup, 0.9340\n",
      "High_Cheekbones, 0.8401\n",
      "Male, 0.9781\n",
      "Mouth_Slightly_Open, 0.7860\n",
      "Mustache, 0.9101\n",
      "Narrow_Eyes, 0.4376\n",
      "No_Beard, 0.7962\n",
      "Oval_Face, 0.4810\n",
      "Pale_Skin, 0.5002\n",
      "Pointy_Nose, 0.3336\n",
      "Receding_Hairline, 0.5225\n",
      "Rosy_Cheeks, 0.8064\n",
      "Sideburns, 0.7043\n",
      "Smiling, 0.8949\n",
      "Straight_Hair, 0.5862\n",
      "Wavy_Hair, 0.5825\n",
      "Wearing_Earrings, 0.8934\n",
      "Wearing_Hat, 0.8974\n",
      "Wearing_Lipstick, 0.9301\n",
      "Wearing_Necklace, 0.8057\n",
      "Wearing_Necktie, 0.6875\n",
      "Young, 0.7526\n"
     ]
    }
   ],
   "source": [
    "new_accuracies = get_accuracies(new_Y_predicted_rounded, new_Y_gt)\n",
    "print('-------------------- test accuracies -------------------------')\n",
    "print('mean: {:.04f}'.format(np.mean(new_accuracies)))\n",
    "for i in range(len(labels)):\n",
    "    print('{}, {:.04f}'.format(labels[i], new_accuracies[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_labels = ['asianindian', 'eastasian', 'african', 'latino', 'caucasian']\n",
    "\n",
    "race_dict = {}\n",
    "def extract_race_dict(predicted_rounded, predicted, img_paths, race_dict):\n",
    "    for i in range(len(predicted)):\n",
    "        one_of_races = False\n",
    "        for j in range(5):\n",
    "            if predicted_rounded[i][j+1] == 1:\n",
    "                one_of_races = True\n",
    "                if race_labels[j] not in race_dict:\n",
    "                    race_dict[race_labels[j]] = []\n",
    "                race_dict[race_labels[j]].append(img_paths[i])\n",
    "        if one_of_races == False:\n",
    "            j = np.argmax(predicted[i][1:6])\n",
    "            if race_labels[j] not in race_dict:\n",
    "                race_dict[race_labels[j]] = []\n",
    "            race_dict[race_labels[j]].append(img_paths[i])\n",
    "\n",
    "extract_race_dict(train_Y_predicted_rounded, train_Y_predicted, train_img_paths, race_dict)\n",
    "extract_race_dict(val_Y_predicted_rounded, val_Y_predicted, val_img_paths, race_dict)\n",
    "extract_race_dict(test_Y_predicted_rounded, test_Y_predicted, test_img_paths, race_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(race_dict, open('race_dict.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19867, 45)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Y_predicted_rounded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
